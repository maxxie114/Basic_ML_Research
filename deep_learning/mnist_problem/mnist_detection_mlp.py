# -*- coding: utf-8 -*-
"""
ä¸€ä¸ªç®€å•çš„æ‰‹å†™æ•°å­—è¯†åˆ«å°é¡¹ç›®ï¼ˆMLP ç‰ˆæœ¬ï¼Œä¸”å¯¼å‡ºçš„å›¾ç‰‡æ ‡ç­¾ä¸é‡å¤ï¼‰ï¼š

1. ä½¿ç”¨æœ¬åœ°çš„ `mnist_train_small.csv`ï¼ˆ28x28 åƒç´ ç°åº¦å›¾æŒ‰è¡Œå±•å¼€ï¼‰
2. ä½¿ç”¨ MLPClassifierï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰è¿›è¡Œæ•°å­—åˆ†ç±»
3. åœ¨æµ‹è¯•é›†ä¸Šç»˜åˆ¶æ··æ·†çŸ©é˜µè¯„ä¼°åˆ†ç±»æ•ˆæœ
4. ä»æµ‹è¯•é›†ä¸­é€‰å–è‹¥å¹²å¼ å›¾ç‰‡ï¼Œæ ¹æ®â€œé¢„æµ‹ç»“æœ.pngâ€å‘½åå¯¼å‡º
5. âœ… æ–°é€»è¾‘ï¼šåªå¯¼å‡ºâ€œé¢„æµ‹ç»“æœæ ‡ç­¾äº’ä¸ç›¸åŒâ€çš„å›¾ç‰‡ï¼Œæ¯”å¦‚ 1.pngã€3.pngã€4.png
6. å¯¼å‡ºçš„ PNG ä¼šè¢«æ”¾å¤§åˆ° 128x128 åƒç´ ï¼Œæ›´å®¹æ˜“çœ‹æ¸…æ•°å­—

éœ€è¦å®‰è£…çš„åº“ï¼ˆåœ¨ç»ˆç«¯/å‘½ä»¤è¡Œé‡Œè¿è¡Œï¼‰ï¼š
    pip install scikit-learn matplotlib numpy pillow
"""

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.neural_network import MLPClassifier  # ä½¿ç”¨ MLP ç¥ç»ç½‘ç»œ
from PIL import Image  # ç”¨æ¥æŠŠ 8x8 å›¾æ”¾å¤§å¹¶ä¿å­˜ä¸º PNG


def main():
    # 1. åŠ è½½æ•°æ®é›†
    print("ğŸ“¥ æ­£åœ¨ä» mnist_train_small.csv åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†...")
    csv_path = Path(__file__).with_name("mnist_train_small.csv")
    if not csv_path.exists():
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ° {csv_path}ï¼Œè¯·ç¡®è®¤æ•°æ®æ–‡ä»¶å­˜åœ¨ã€‚"
        )

    data = np.loadtxt(csv_path, delimiter=",", dtype=np.float32)

    # å‰ä¸€åˆ—æ˜¯æ ‡ç­¾ï¼ˆæ•°å­— 0~9ï¼‰ï¼Œå…¶ä½™ 784 åˆ—æ˜¯ 28x28 ç°åº¦å›¾å±•å¹³åçš„åƒç´ 
    y = data[:, 0].astype(int)
    X = data[:, 1:]
    images = X.reshape(-1, 28, 28)

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œä¸€å…±åŒ…å« {len(X)} ä¸ªæ ·æœ¬ã€‚")

    # 2. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆåŒæ—¶åˆ’åˆ† imagesï¼Œä¿è¯ç´¢å¼•å¯¹åº”ï¼‰
    print("ğŸ”€ æ­£åœ¨åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test, img_train, img_test = train_test_split(
        X,
        y,
        images,
        test_size=0.3,
        random_state=42,
        stratify=y,  # æŒ‰ç±»åˆ«åˆ†å±‚æŠ½æ ·ï¼Œä¿è¯æ¯”ä¾‹å¤§è‡´ä¸€è‡´
    )
    print(f"âœ… è®­ç»ƒé›†å¤§å°ï¼š{len(X_train)}ï¼Œæµ‹è¯•é›†å¤§å°ï¼š{len(X_test)}")

    # 3. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå¯¹ MLP å¾ˆé‡è¦ï¼‰
    print("ğŸ“ æ­£åœ¨å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    print("âœ… æ ‡å‡†åŒ–å®Œæˆã€‚")

    # 4. é€šè¿‡ç½‘æ ¼æœç´¢å¯»æ‰¾æ›´ä¼˜çš„è¶…å‚æ•°
    base_clf = MLPClassifier(max_iter=200, random_state=42)
    param_grid = {
        "hidden_layer_sizes": [(128,), (128, 64), (256, 128)],
        "alpha": [1e-3, 1e-2],
        "learning_rate_init": [1e-3, 5e-4],
    }
    search = GridSearchCV(
        estimator=base_clf,
        param_grid=param_grid,
        cv=3,
        n_jobs=-1,
        verbose=1,
    )

    print("ğŸ” æ­£åœ¨è¿›è¡Œ GridSearchCV è¶…å‚æ•°æœç´¢...")
    search.fit(X_train_scaled, y_train)
    clf = search.best_estimator_
    print(
        "âœ… è¶…å‚æ•°æœç´¢å®Œæˆï¼š",
        search.best_params_,
        f"ï¼ˆå‡å€¼éªŒè¯å¾—åˆ†ï¼š{search.best_score_:.4f}ï¼‰",
    )

    print("ğŸ¤– ä½¿ç”¨æœ€ä½³è¶…å‚æ•°é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    clf.fit(X_train_scaled, y_train)
    print("âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

    # é¢å¤–ä¿å­˜ä¸€æ¬¡æŸå¤±æ›²çº¿ï¼Œæ–¹ä¾¿è§‚å¯Ÿæ¨¡å‹æ”¶æ•›æƒ…å†µ
    if getattr(clf, "loss_curve_", None):
        print("ğŸ“‰ æ­£åœ¨ä¿å­˜ loss æ›²çº¿å›¾ loss_curve.png ...")
        plt.figure(figsize=(8, 4))
        plt.plot(
            range(1, len(clf.loss_curve_) + 1),
            clf.loss_curve_,
            marker="o",
            linewidth=1.5,
            markersize=4,
        )
        plt.title("MLP Training Loss Curve")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig("loss_curve.png", dpi=150)
        plt.close()
        print("âœ… loss æ›²çº¿å›¾å·²ä¿å­˜ä¸º loss_curve.pngã€‚")

    # 5. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹è¡¨ç°
    print("ğŸ“Š æ­£åœ¨è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°...")
    y_pred = clf.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    print(f"âœ… æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ä¸ºï¼š{acc:.2f}")

    # 6. ç”Ÿæˆå¹¶ä¿å­˜æ··æ·†çŸ©é˜µ
    print("ğŸ§® æ­£åœ¨ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–...")
    cm = confusion_matrix(y_test, y_pred, labels=range(10))
    fig, ax = plt.subplots(figsize=(8, 6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(10))
    disp.plot(ax=ax, cmap="Blues", colorbar=False)
    ax.set_title("Digits classification with MLP (Confusion Matrix)")
    fig.tight_layout()

    confusion_path = "confusion_matrix.png"
    fig.savefig(confusion_path, dpi=200)
    backend = plt.get_backend().lower()
    if "agg" not in backend:
        plt.show()
        plt.close(fig)
        print(
            f"âœ… æ··æ·†çŸ©é˜µå·²æ˜¾ç¤ºï¼Œå¹¶ä¿å­˜ä¸º {confusion_path}ã€‚"
        )
    else:
        plt.close(fig)
        print(
            f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜ä¸º {confusion_path}ï¼ˆå½“å‰åç«¯ï¼š{backend}ï¼‰ã€‚"
        )

    # 7. ä»æµ‹è¯•é›†ä¸­é€‰å–å›¾ç‰‡ï¼Œæ”¾å¤§åä¿å­˜ä¸ºâ€œé¢„æµ‹ç»“æœ.pngâ€
    #    âœ… æ–°é€»è¾‘ï¼šåªä¿å­˜â€œé¢„æµ‹ç»“æœä¸é‡å¤â€çš„å›¾ç‰‡
    print("ğŸ’¾ æ­£åœ¨ä»æµ‹è¯•é›†ä¸­é€‰å–å›¾ç‰‡å¹¶ä¿å­˜ä¸ºæ”¾å¤§åçš„ PNG æ–‡ä»¶ï¼ˆé¢„æµ‹ç»“æœäº’ä¸é‡å¤ï¼‰...")
    num_to_save = 5         # å¸Œæœ›ä¿å­˜çš„â€œä¸åŒæ•°å­—â€çš„æ•°é‡
    scale = 16               # æ”¾å¤§å€æ•°ï¼š8 * 16 = 128ï¼Œæ‰€ä»¥è¾“å‡º 128x128 åƒç´ 

    saved_labels = set()     # å·²ç»ä¿å­˜è¿‡çš„æ•°å­—æ ‡ç­¾
    saved_count = 0

    rng = np.random.default_rng()
    shuffled_indices = rng.permutation(len(img_test))

    for idx in shuffled_indices:
        img = img_test[idx]
        pred_label = y_pred[idx]
        # å¦‚æœè¿™ä¸ªæ•°å­—å·²ç»ä¿å­˜è¿‡äº†ï¼Œå°±è·³è¿‡
        if pred_label in saved_labels:
            continue

        # CSV ä¸­çš„åƒç´ èŒƒå›´æ˜¯ 0~255ï¼Œè¿™é‡Œä¿æŒåŒæ ·çš„èŒƒå›´
        max_val = img.max()
        if max_val == 0:
            img_norm = img
        else:
            img_norm = img / max_val * 255.0

        img_uint8 = img_norm.astype(np.uint8)  # Pillow éœ€è¦ uint8 æ ¼å¼

        # ç”Ÿæˆæ”¾å¤§åçš„å›¾åƒ
        pil_img = Image.fromarray(img_uint8, mode="L")  # "L" è¡¨ç¤ºç°åº¦å›¾
        new_size = (img_uint8.shape[1] * scale, img_uint8.shape[0] * scale)
        big_img = pil_img.resize(new_size, Image.NEAREST)  # NEAREST ä¿ç•™åƒç´ å—é£æ ¼

        filename = f"{pred_label}.png"

        big_img.save(filename)

        saved_labels.add(pred_label)
        saved_count += 1

        print(
            f"âœ… å·²ä¿å­˜ç¬¬ {saved_count} å¼ å›¾ç‰‡ï¼Œå¯¹åº”çš„é¢„æµ‹ç»“æœä¸º {pred_label}ï¼Œ"
            f"æ–‡ä»¶åï¼š{filename}ï¼Œå°ºå¯¸ï¼š{new_size[0]}x{new_size[1]} åƒç´ "
        )

        # å¦‚æœå·²ç»ä¿å­˜å¤Ÿäº†ï¼Œå°±åœæ­¢
        if saved_count >= num_to_save:
            break

    if saved_count < num_to_save:
        print(
            f"âš  è¯´æ˜ï¼šæµ‹è¯•é›†ä¸­æ¨¡å‹é¢„æµ‹åˆ°çš„ä¸åŒæ•°å­—ä¸€å…±åªæœ‰ {saved_count} ç§ï¼Œ"
            f"å°‘äºæœŸæœ›çš„ {num_to_save} ç§ï¼Œå› æ­¤åªä¿å­˜äº† {saved_count} å¼ å›¾ç‰‡ã€‚"
        )

    print("ğŸ‰ æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæ¯•ï¼Œå¯ä»¥åœ¨å½“å‰é¡¹ç›®ç›®å½•ä¸‹çœ‹åˆ°ç”Ÿæˆçš„ PNG å›¾ç‰‡ã€‚")


if __name__ == "__main__":
    main()
